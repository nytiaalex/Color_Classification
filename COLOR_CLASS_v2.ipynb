{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polar-marsh",
   "metadata": {},
   "source": [
    "<font size='4'><b>Color Classification</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-longer",
   "metadata": {},
   "source": [
    "In this project we want to create a Deep learning model for car color classification. Along the project we will first prepare the data for training and testing, then we will fit our data into a convolutional network model, and lastly we will feed our trained model new images and predict their classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-poultry",
   "metadata": {},
   "source": [
    "We import the important modules for our data preparations. We use numpy for matrix and array operations, Matplotlib and cv2 for image manipulations and us will use OS for some file pathing interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-publisher",
   "metadata": {},
   "source": [
    "<font size='3'><b>1. Data preparation</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-premium",
   "metadata": {},
   "source": [
    "Before training our model we need to prepare the data. The data are images of different cars classed by their color, and placed in different folders in function of their colors. There are 5 different classes of color (5 folders), white, red, green, blue and black. And each class (folder) contains 640 images of cars. (Exception: Green, 420 images). In total there are 2980 image samples available for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-stomach",
   "metadata": {},
   "source": [
    "The different class folders are placed inside DATADIR file \"DATASET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"DATASET\"\n",
    "CATEDIR = [\"White\",\"Red\",\"Green\",\"Blue\",\"Black\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-spectrum",
   "metadata": {},
   "source": [
    "In order to access the image inside a class, we use 2 'for' loops. The first loop gives access to the DATADIR, and each class folder CATEDIR. The second loop gives access to the images inside the class folders.\n",
    "The image in itself is useless as a data, we use cv2.imread in order to translate the image into an array. And with matplotlib imshow we can have a display of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_folder in CATEDIR:\n",
    "# access to the inside DATASET folder containing the different class folders\n",
    "    path = DATADIR+\"/\"+class_folder\n",
    "    for img in os.listdir(path):\n",
    "    # access to the inside of each class folder, where the images samples are\n",
    "        img_path = path+\"/\"+img\n",
    "        # transform image into an array\n",
    "        img_array = cv2.imread(img_path)\n",
    "        plt.imshow(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "        plt.show\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-framework",
   "metadata": {},
   "source": [
    "The image as an array has 3 parameters, 2 parameters for the size, the weight and the height, and one parameter the number of color layers (3, in our case, referencing to RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image shape: \", img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image array: \", img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-teddy",
   "metadata": {},
   "source": [
    "In certain cases the images from our dataset can come with different size. In order to have a dataset uniform, we will resize the images using cv2.resize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 100\n",
    "\n",
    "new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE), 3)\n",
    "plt.imshow(cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-trainer",
   "metadata": {},
   "source": [
    "The resized image retains all its features, and the image is still recognizable.\n",
    "The shape of the new image array also retains the 3 parameters, but with different values for the 1st and 2nd parameters, corresponding to the weight and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"new image shape: \", new_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"new image array: \", new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-patrol",
   "metadata": {},
   "source": [
    "<font size='3'><b>2. Creating the training data</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-blank",
   "metadata": {},
   "source": [
    "We have now all the necessary tools, to create the training data for our neural network model. Same as previously, we use the double 'for' loops in order to access and retrieve the image samples inside each class folder, and we want to assign indexes to each of our samples data folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-dover",
   "metadata": {},
   "source": [
    "We also transform each image into an array, resize them, and save them alongside their class index, inside training_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "IMG_SIZE = 100\n",
    "\n",
    "def create_training_data():\n",
    "    for class_folder in CATEDIR:\n",
    "        path = DATADIR+\"/\"+class_folder\n",
    "        # give an index to each class folder\n",
    "        class_index = CATEDIR.index(class_folder)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(path+\"/\"+img)\n",
    "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE), 3)\n",
    "            training_data.append([new_array, class_index])\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-climate",
   "metadata": {},
   "source": [
    "We can observe the length of training_data corresponding to the total number of image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of image samples: \", len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-methodology",
   "metadata": {},
   "source": [
    "By displaying some of the samples in our training data, we observe how our data samples are arranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in training_data[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the images from the classes inside training_data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for images, classes in training_data:\n",
    "    X.append(images)\n",
    "    y.append(classes)\n",
    "    \n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-incidence",
   "metadata": {},
   "source": [
    "We can see that the image sample are ordered by their classes. All the images of white car first, followed by the red one, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the first 10 image samples of our training data\n",
    "fig = plt.figure(figsize=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "for i,img in enumerate(X[:10]):\n",
    "    fig.add_subplot(4,5, i+1)\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-characteristic",
   "metadata": {},
   "source": [
    "In order to be more efficient in the training process, we will shuffle our image samples inside training_data, by importing the module random.\n",
    "With random.shuffle we can shuffle the order of the image samples inside training data randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the images from the classes inside training_data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for images, classes in training_data:\n",
    "    X.append(images)\n",
    "    y.append(classes)\n",
    "    \n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-style",
   "metadata": {},
   "source": [
    "We can see that the image sample are ordered randomly inside training_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "for i,img in enumerate(X[:10]):\n",
    "    fig.add_subplot(4,5, i+1)\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-amount",
   "metadata": {},
   "source": [
    "After separating the images and the indexes, from the training_data, we can observe the both of them have the same length, corresponding to the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"output shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-disabled",
   "metadata": {},
   "source": [
    "<font size='3'><b>3. Creating the training model</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-broadcast",
   "metadata": {},
   "source": [
    "We now have the training data needed to train a neural network model with. Next we need to create the neural network, and we choose to use keras and tensorflow. We import the different keras modules needed for creating a neural network model: Sequential to define the type of model; Dense, Conv2D refers respectively to a deep neural network and a convolutional network; MaxPooling2D is used by the convolutional network; and Flatten is used in order to transform a 2D convolutional network into a fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-chaos",
   "metadata": {},
   "source": [
    "We have our images, data samples in X and their classes in y. In order to understand how well our model is doing with different samples not used for training, we split the data into training data and validation data, with the use of train_test_split from sklearn. We split the data into 70% training data and 30% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,\n",
    "                                                   shuffle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training model input shape: \", X_train.shape, \"\\n\")\n",
    "print(\"testing model input shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training model input: \", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, len(CATEDIR))\n",
    "y_test = to_categorical(y_test, len(CATEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training model output shape: \", y_train.shape, \"\\n\")\n",
    "print(\"testing model output shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training model output: \", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-viewer",
   "metadata": {},
   "source": [
    "Now, we want to create our training model, we decided to create a convolutional neural network model which is more adapted for image recognition. <br>\n",
    "First, we add the input layer with the input_shape corresponding to the shape of the images, and we specify the layer activation rule. <br>\n",
    "We add more convolutional layers with the activation rule, before flattening the model into a fully connected dense layer, and finally adding the output layer.<br>\n",
    "We have a classification problem, the activation softmax for the output layer is more adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), input_shape = (IMG_SIZE, IMG_SIZE, 3), \n",
    "                 activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "\n",
    "model.add(Dense(len(CATEDIR), activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-central",
   "metadata": {},
   "source": [
    "After adding the layers we compile them into our model, and using the Adam algorithm as the optimizer. As a classification problem, the loss function of the model can be obtained with the categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\", \n",
    "              optimizer = \"adam\", \n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-three",
   "metadata": {},
   "source": [
    "Now our training model is created and can be used to train with the different image samples, we can see the structure of our model using the model.summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-crest",
   "metadata": {},
   "source": [
    "We fit the training data, which mean training our model with the image sample. And we specify the batch_size, corresponding to how many samples at a time are trained, then we specify the number of epochs, which is how much time the model are going to run throughout the sample, and we specify the validation data, to understand how our model fare with image different from the training image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 64, epochs = 10, \n",
    "          validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-marathon",
   "metadata": {},
   "source": [
    "<b>Small conclusion:</b><br>\n",
    "We can observe that our model is relatively doing well, despite the number of samples, we obtain around 90% accuracy for the training, and 85% - 87% for the validation. In order to have a better understanding of the result, we plot the evolution of the accuracy and loss of the training and validation, in function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-butter",
   "metadata": {},
   "source": [
    "With the graphics, we may observe some indication of overfitting which can be explained by the simplicity of our model, and our technique of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_v1_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-dating",
   "metadata": {},
   "source": [
    "<font size='3'><b>4. Image prediction</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-houston",
   "metadata": {},
   "source": [
    "We have created a model capable of predicting color of a car, and we want to test the model using completely different images from the training or validation data. Those images aren't arranged inside the class folder so they don't posses a class label, we want to use our model to classify them (meaning determining their color for us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDDIR = \"PREDICT\"\n",
    "CATEDIR = [\"White\",\"Red\",\"Green\",\"Blue\",\"Black\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = []\n",
    "IMG_SIZE = 100\n",
    "\n",
    "for img in os.listdir(PREDDIR):\n",
    "    img_array = cv2.imread(PREDDIR+\"/\"+img)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE), 3)\n",
    "    prediction_data.append(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.array(prediction_data).reshape(-1, IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-mortality",
   "metadata": {},
   "source": [
    "We reload our saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_v1_1.h5\", compile = True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-drill",
   "metadata": {},
   "source": [
    "We predict the classes of the new input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-export",
   "metadata": {},
   "source": [
    "In order to see the accuracy of the model, bellow are the images that we want to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "for i,img in enumerate(X_pred):\n",
    "    fig.add_subplot(4,5, i+1)\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-savannah",
   "metadata": {},
   "source": [
    "We have bellow, the prediction from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    result = y_pred[i,:]\n",
    "    print(\"image\", i)\n",
    "    for j in range(5):\n",
    "        if result[j] == 1.0:\n",
    "            print(\"color: \", CATEDIR[j], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-affiliation",
   "metadata": {},
   "source": [
    "<b>Small conclusion:</b><br>\n",
    "We can observe our model is capable to classify the color of a car, which was our objective in this project. <br>\n",
    "The prediction of our model isn't 100% accurate, but the result can be perfected by tunning our neural network model and tweaking with the training technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-warner",
   "metadata": {},
   "source": [
    "<font size='3'><b>5. Different training model / Training technique</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-spokesman",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
